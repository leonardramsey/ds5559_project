{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synopsis\n",
    "\n",
    "Create an LDA of 20news corpus using MALLET."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_file = '20news_01.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 15\n",
    "num_iters = 1000\n",
    "show_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re\n",
    "import random\n",
    "import textman as tx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pragmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = pd.read_csv(src_file, sep='\\t')\n",
    "docs = docs.set_index('doc_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Convert corpus to tokens and vocab\n",
    "\n",
    "We use a function from TextMan, a bespoke library that incorporates the text processing routines used in earlier notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, vocab = tx.create_tokens_and_vocab(docs, src_col='doc_content')\n",
    "tokens['token_num'] = tokens.groupby(['doc_id']).cumcount()\n",
    "tokens = tokens.reset_index()[['doc_id','token_num','term_id']]\n",
    "tokens = tokens[tokens.term_id.isin(vocab[vocab.go].index)]\n",
    "tokens = tokens.set_index(['doc_id','token_num'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add term strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens['term_str'] = tokens.term_id.map(vocab.term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>term_id</th>\n",
       "      <th>term_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_id</th>\n",
       "      <th>token_num</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">76209</th>\n",
       "      <th>0</th>\n",
       "      <td>4557</td>\n",
       "      <td>people</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5848</td>\n",
       "      <td>sure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4713</td>\n",
       "      <td>posts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2671</td>\n",
       "      <td>forwarded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5882</td>\n",
       "      <td>system</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  term_id   term_str\n",
       "doc_id token_num                    \n",
       "76209  0             4557     people\n",
       "       1             5848       sure\n",
       "       2             4713      posts\n",
       "       3             2671  forwarded\n",
       "       4             5882     system"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Remove insignificant words\n",
    "\n",
    "We use SKlearn's TFIDF vectorizor to quicky get a TFIDF vector space, which we use only to filter the words in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=1, stop_words='english', token_pattern=r'[A-Za-z][A-Za-z][A-Za-z]+')\n",
    "X = vectorizer.fit_transform(docs.doc_content.values.tolist())\n",
    "v = pd.DataFrame(vectorizer.get_feature_names(), columns=['term_str'])\n",
    "v['idf'] = vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_str</th>\n",
       "      <th>idf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aaa</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3491</th>\n",
       "      <td>nicely</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3488</th>\n",
       "      <td>nhlpa</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3486</th>\n",
       "      <td>nga</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3484</th>\n",
       "      <td>newswriter</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>newsweek</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3481</th>\n",
       "      <td>newspaper</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3480</th>\n",
       "      <td>newsgroups</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3478</th>\n",
       "      <td>newsbytes</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3476</th>\n",
       "      <td>newly</td>\n",
       "      <td>4.921973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        term_str       idf\n",
       "0            aaa  4.921973\n",
       "3491      nicely  4.921973\n",
       "3488       nhlpa  4.921973\n",
       "3486         nga  4.921973\n",
       "3484  newswriter  4.921973\n",
       "3483    newsweek  4.921973\n",
       "3481   newspaper  4.921973\n",
       "3480  newsgroups  4.921973\n",
       "3478   newsbytes  4.921973\n",
       "3476       newly  4.921973"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.sort_values('idf', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take only the most significant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 4.5\n",
    "v = v[v.idf > cutoff].sort_values('idf', ascending=False).sample(1000)\n",
    "my_v = v.term_str.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[tokens.term_str.isin(my_v)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = vocab[vocab.term.isin(my_v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export corpus for MALLET "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = tx.gather_tokens(tokens, level=0, col='term_str')\\\n",
    "    .reset_index().rename(columns={'term_str':'doc_content'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>doc_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20567</td>\n",
       "      <td>hell hell hell hell atheists hell atheists abu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20758</td>\n",
       "      <td>comments apologists theology theology approach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20859</td>\n",
       "      <td>esther female lunch lunch lunch lunch sub piec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20888</td>\n",
       "      <td>belief belief belief belief religion religion ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20910</td>\n",
       "      <td>goer goer goer goer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                        doc_content\n",
       "0   20567  hell hell hell hell atheists hell atheists abu...\n",
       "1   20758  comments apologists theology theology approach...\n",
       "2   20859  esther female lunch lunch lunch lunch sub piec...\n",
       "3   20888  belief belief belief belief religion religion ...\n",
       "4   20910                                goer goer goer goer"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_csv('20news-corpus.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized command: \n",
      "Mallet 2.0 commands: \n",
      "\n",
      "  import-dir         load the contents of a directory into mallet instances (one per file)\n",
      "  import-file        load a single file into mallet instances (one per line)\n",
      "  import-svmlight    load SVMLight format data files into Mallet instances\n",
      "  info               get information about Mallet instances\n",
      "  train-classifier   train a classifier from Mallet data files\n",
      "  classify-dir       classify data from a single file with a saved classifier\n",
      "  classify-file      classify the contents of a directory with a saved classifier\n",
      "  classify-svmlight  classify data from a single file in SVMLight format\n",
      "  train-topics       train a topic model from Mallet data files\n",
      "  infer-topics       use a trained topic model to infer topics for new documents\n",
      "  evaluate-topics    estimate the probability of new documents under a trained model\n",
      "  prune              remove features based on frequency or information gain\n",
      "  split              divide data into testing, training, and validation portions\n",
      "  bulk-load          for big input files, efficiently prune vocabulary and import docs\n",
      "\n",
      "Include --help with any option for more information\n"
     ]
    }
   ],
   "source": [
    "!mallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mallet import-file --input 20news-corpus.csv --output 20news-corpus.mallet --keep-sequence TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mallet LDA: 15 topics, 4 topic bits, 1111 topic mask\n",
      "Data loaded.\n",
      "max tokens: 68\n",
      "total tokens: 429\n",
      "<10> LL/token: -4.73179\n",
      "<20> LL/token: -4.49812\n",
      "<30> LL/token: -4.43303\n",
      "<40> LL/token: -4.36491\n",
      "<50> LL/token: -4.36546\n",
      "<60> LL/token: -4.31527\n",
      "<70> LL/token: -4.22946\n",
      "<80> LL/token: -4.26502\n",
      "<90> LL/token: -4.22581\n",
      "\n",
      "0\t0.33333\tajteel barb sysadmin eclipse ida requested interesting alternative \n",
      "1\t0.33333\tbryn vilka den det \n",
      "2\t0.33333\tlunch processor runner centris conservative female survey peace \n",
      "3\t0.33333\tconcept supernatural plurality theology dealer comments abuse \n",
      "4\t0.33333\tyalanci vay org reference columbia dbd hovig peace criminals \n",
      "5\t0.33333\tbelief universe boone straight sox asks goer \n",
      "6\t0.33333\tbank palestinian meaningless umd disk \n",
      "7\t0.33333\tinternational molestation courier modems overwhelming azerbaijan magazine iii comments \n",
      "8\t0.33333\tmon teel boulder newsgroups overwhelming pouring \n",
      "9\t0.33333\tpray piece sub esther malaysia dealer \n",
      "10\t0.33333\tmin det tor cgy powers floppies \n",
      "11\t0.33333\tfredericton binghamton religion messiah koresh solun cup \n",
      "12\t0.33333\tspirit oneness trinitarianism substance approach arius biblical apologists canon \n",
      "13\t0.33333\thell atheists reds \n",
      "14\t0.33333\tcongress reasoning higher arguments bush current record somebody barry abuse \n",
      "\n",
      "<100> LL/token: -4.21784\n",
      "<110> LL/token: -4.24376\n",
      "<120> LL/token: -4.16347\n",
      "<130> LL/token: -4.17632\n",
      "<140> LL/token: -4.14696\n",
      "<150> LL/token: -4.16891\n",
      "<160> LL/token: -4.12356\n",
      "<170> LL/token: -4.11194\n",
      "<180> LL/token: -4.11675\n",
      "<190> LL/token: -4.12298\n",
      "\n",
      "0\t0.33333\tajteel barb sysadmin ida \n",
      "1\t0.33333\tbryn vilka den umd goer floppies \n",
      "2\t0.33333\tinteresting dealer processor molestation overwhelming barry centris \n",
      "3\t0.33333\toneness concept arius plurality theology runner international magazine \n",
      "4\t0.33333\tyalanci vay org reference columbia dbd hovig peace criminals sub \n",
      "5\t0.33333\tbelief universe supernatural canon asks survey iii reasoning \n",
      "6\t0.33333\tbank palestinian meaningless reds female disk \n",
      "7\t0.33333\tboone straight sox international azerbaijan pouring \n",
      "8\t0.33333\tmon teel eclipse boulder requested newsgroups alternative conservative \n",
      "9\t0.33333\tpray lunch piece esther sub peace \n",
      "10\t0.33333\tdet min tor cgy modems \n",
      "11\t0.33333\tfredericton binghamton religion messiah koresh courier cup \n",
      "12\t0.33333\tspirit trinitarianism substance approach biblical apologists comments \n",
      "13\t0.33333\thell atheists malaysia \n",
      "14\t0.33333\tcongress higher arguments reasoning bush current abuse record somebody powers solun \n",
      "\n",
      "<200> LL/token: -4.19569\n",
      "<210> LL/token: -4.14205\n",
      "<220> LL/token: -4.20785\n",
      "<230> LL/token: -4.21625\n",
      "<240> LL/token: -4.16891\n",
      "<250> LL/token: -4.17637\n",
      "<260> LL/token: -4.14938\n",
      "<270> LL/token: -4.14615\n",
      "<280> LL/token: -4.1738\n",
      "<290> LL/token: -4.17892\n",
      "\n",
      "0\t0.33333\tajteel mon sysadmin eclipse ida requested newsgroups comments \n",
      "1\t0.33333\tbryn vilka processor den umd floppies disk \n",
      "2\t0.33333\tmessiah koresh molestation overwhelming modems azerbaijan \n",
      "3\t0.33333\tconcept universe arius supernatural solun pouring cup \n",
      "4\t0.33333\tyalanci vay org reference peace columbia dbd hovig criminals \n",
      "5\t0.33333\tbelief meaningless reds courier conservative asks \n",
      "6\t0.33333\tbank palestinian dealer centris goer \n",
      "7\t0.33333\tboone international straight sox canon magazine iii \n",
      "8\t0.33333\tteel barb boulder interesting alternative supernatural \n",
      "9\t0.33333\tpray lunch piece sub esther record female \n",
      "10\t0.33333\tdet min tor cgy malaysia \n",
      "11\t0.33333\tfredericton binghamton religion barry runner \n",
      "12\t0.33333\tspirit oneness trinitarianism substance approach biblical apologists plurality theology survey comments \n",
      "13\t0.33333\thell atheists \n",
      "14\t0.33333\tcongress reasoning higher arguments bush current abuse somebody powers \n",
      "\n",
      "[beta: 0.01101] \n",
      "<300> LL/token: -4.04367\n",
      "<310> LL/token: -4.01507\n",
      "<320> LL/token: -3.98154\n",
      "<330> LL/token: -3.96679\n",
      "<340> LL/token: -4.02058\n",
      "<350> LL/token: -4.0116\n",
      "<360> LL/token: -4.01682\n",
      "<370> LL/token: -3.97811\n",
      "<380> LL/token: -4.02433\n",
      "<390> LL/token: -3.99382\n",
      "\n",
      "0\t0.10373\tajteel mon sysadmin eclipse ida newsgroups comments \n",
      "1\t0.12542\tbryn vilka den azerbaijan comments \n",
      "2\t0.19802\treligion messiah koresh meaningless reds umd overwhelming solun pouring \n",
      "3\t0.14752\tuniverse alternative supernatural molestation asks \n",
      "4\t0.11963\tyalanci vay org reference peace columbia dbd hovig criminals \n",
      "5\t0.22716\tbelief courier modems conservative disk reasoning \n",
      "6\t0.2016\tbank palestinian dealer centris goer female survey \n",
      "7\t0.23154\tboone international straight processor barry sox magazine floppies \n",
      "8\t0.1512\tteel barb boulder requested interesting malaysia overwhelming \n",
      "9\t0.15984\tpray lunch piece sub esther \n",
      "10\t0.16033\tdet min tor cgy iii \n",
      "11\t0.19656\tfredericton binghamton runner cup \n",
      "12\t0.12189\tspirit oneness concept trinitarianism substance approach arius biblical apologists plurality theology canon \n",
      "13\t0.12818\thell atheists abuse \n",
      "14\t0.15064\tcongress higher arguments reasoning bush current record somebody abuse powers \n",
      "\n",
      "[beta: 0.0089] \n",
      "<400> LL/token: -3.95252\n",
      "<410> LL/token: -3.92062\n",
      "<420> LL/token: -3.99857\n",
      "<430> LL/token: -3.90788\n",
      "<440> LL/token: -3.93409\n",
      "<450> LL/token: -3.9124\n",
      "<460> LL/token: -3.95782\n",
      "<470> LL/token: -3.92518\n",
      "<480> LL/token: -3.92759\n",
      "<490> LL/token: -3.93091\n",
      "\n",
      "0\t0.04889\tajteel sysadmin eclipse ida interesting goer overwhelming comments \n",
      "1\t0.08812\tbryn vilka den det \n",
      "2\t0.10573\tmessiah koresh meaningless \n",
      "3\t0.07622\tuniverse molestation malaysia umd asks supernatural overwhelming floppies alternative comments \n",
      "4\t0.04474\tyalanci vay org reference columbia dbd hovig peace criminals disk sub \n",
      "5\t0.15413\tbelief religion reds courier modems conservative \n",
      "6\t0.09148\tbank palestinian dealer centris pouring female \n",
      "7\t0.12967\tboone international straight processor barry sox azerbaijan magazine \n",
      "8\t0.07057\tmon teel barb boulder requested newsgroups alternative \n",
      "9\t0.07624\tpray lunch piece esther sub peace female \n",
      "10\t0.07372\tmin det tor cgy solun \n",
      "11\t0.09608\tfredericton binghamton survey cup \n",
      "12\t0.0578\tspirit oneness concept trinitarianism substance approach arius biblical apologists plurality theology canon iii supernatural \n",
      "13\t0.05038\thell atheists abuse \n",
      "14\t0.08502\tcongress reasoning higher arguments bush current record somebody runner abuse powers \n",
      "\n",
      "[beta: 0.01036] \n",
      "<500> LL/token: -3.91521\n",
      "<510> LL/token: -3.89995\n",
      "<520> LL/token: -3.87312\n",
      "<530> LL/token: -3.84064\n",
      "<540> LL/token: -3.86968\n",
      "<550> LL/token: -3.90643\n",
      "<560> LL/token: -3.87075\n",
      "<570> LL/token: -3.85613\n",
      "<580> LL/token: -3.88211\n",
      "<590> LL/token: -3.88609\n",
      "\n",
      "0\t0.03333\tajteel sysadmin eclipse ida requested overwhelming umd interesting comments \n",
      "1\t0.03004\tbryn vilka den disk \n",
      "2\t0.06921\treligion messiah koresh \n",
      "3\t0.08089\tuniverse supernatural asks belief pouring alternative \n",
      "4\t0.03441\tyalanci vay org reference columbia dbd hovig peace criminals sub \n",
      "5\t0.10783\tbelief molestation meaningless runner reds courier modems \n",
      "6\t0.05147\tbank palestinian dealer processor centris survey org solun female \n",
      "7\t0.10611\tboone international straight interesting barry sox goer azerbaijan magazine \n",
      "8\t0.03129\tmon teel barb boulder newsgroups alternative \n",
      "9\t0.03946\tpray lunch esther piece sub conservative peace female \n",
      "10\t0.03852\tdet min tor cgy \n",
      "11\t0.02767\tfredericton binghamton cup \n",
      "12\t0.03404\tspirit oneness concept trinitarianism substance approach arius biblical apologists plurality theology floppies iii comments \n",
      "13\t0.01617\thell atheists canon abuse \n",
      "14\t0.0518\tcongress reasoning higher arguments bush current record somebody malaysia abuse powers piece \n",
      "\n",
      "[beta: 0.01305] \n",
      "<600> LL/token: -3.83214\n",
      "<610> LL/token: -3.83346\n",
      "<620> LL/token: -3.85331\n",
      "<630> LL/token: -3.87962\n",
      "<640> LL/token: -3.83346\n",
      "<650> LL/token: -3.81245\n",
      "<660> LL/token: -3.80573\n",
      "<670> LL/token: -3.79688\n",
      "<680> LL/token: -3.85285\n",
      "<690> LL/token: -3.83108\n",
      "\n",
      "0\t0.01477\tajteel mon sysadmin boulder eclipse requested alternative interesting \n",
      "1\t0.02055\tbryn vilka den det \n",
      "2\t0.03122\tbelief religion messiah koresh \n",
      "3\t0.0568\tuniverse supernatural conservative asks survey pouring disk reasoning alternative \n",
      "4\t0.01906\tyalanci vay org reference columbia dbd hovig peace criminals sub \n",
      "5\t0.07659\tmolestation meaningless runner reds malaysia courier modems goer overwhelming \n",
      "6\t0.03645\tbank palestinian dealer centris processor org female \n",
      "7\t0.07595\tboone international straight interesting barry sox azerbaijan magazine \n",
      "8\t0.02204\tteel barb ida newsgroups eclipse overwhelming comments \n",
      "9\t0.02628\tpray lunch piece esther sub peace female \n",
      "10\t0.02999\tmin det tor cgy umd \n",
      "11\t0.01423\tfredericton binghamton floppies cup processor \n",
      "12\t0.02482\tspirit oneness concept trinitarianism substance approach arius biblical apologists plurality theology canon iii comments \n",
      "13\t0.01291\thell atheists abuse \n",
      "14\t0.03967\tcongress higher arguments reasoning bush current record somebody abuse powers solun \n",
      "\n",
      "[beta: 0.00919] \n",
      "<700> LL/token: -3.8632\n",
      "<710> LL/token: -3.86426\n",
      "<720> LL/token: -3.91532\n",
      "<730> LL/token: -3.8397\n",
      "<740> LL/token: -3.87658\n",
      "<750> LL/token: -3.89654\n",
      "<760> LL/token: -3.88834\n",
      "<770> LL/token: -3.84445\n",
      "<780> LL/token: -3.82886\n",
      "<790> LL/token: -3.84816\n",
      "\n",
      "0\t0.01091\tajteel mon barb sysadmin boulder requested newsgroups overwhelming disk \n",
      "1\t0.01332\tbryn vilka den det \n",
      "2\t0.02624\treligion belief messiah koresh \n",
      "3\t0.03223\tuniverse supernatural malaysia asks belief survey alternative comments \n",
      "4\t0.01352\tyalanci vay org reference columbia dbd hovig peace criminals sub \n",
      "5\t0.06267\tbank palestinian molestation meaningless runner reds conservative overwhelming org pouring female \n",
      "6\t0.04517\tdealer processor courier modems centris goer floppies \n",
      "7\t0.04351\tboone international straight interesting barry sox azerbaijan magazine \n",
      "8\t0.01274\tteel eclipse ida alternative interesting comments \n",
      "9\t0.01612\tpray lunch esther piece sub peace female \n",
      "10\t0.01816\tmin det tor cgy umd \n",
      "11\t0.01158\tfredericton binghamton cup \n",
      "12\t0.01817\tspirit oneness concept trinitarianism substance approach arius biblical apologists plurality theology canon iii \n",
      "13\t0.00918\thell atheists abuse \n",
      "14\t0.02477\tcongress reasoning higher arguments bush current record somebody abuse powers solun piece \n",
      "\n",
      "[beta: 0.00701] \n",
      "<800> LL/token: -3.88254\n",
      "<810> LL/token: -3.87015\n",
      "<820> LL/token: -3.871\n",
      "<830> LL/token: -3.86593\n",
      "<840> LL/token: -3.87972\n",
      "<850> LL/token: -3.87591\n",
      "<860> LL/token: -3.89185\n",
      "<870> LL/token: -3.86917\n",
      "<880> LL/token: -3.88081\n",
      "<890> LL/token: -3.89934\n",
      "\n",
      "0\t0.00739\tajteel mon barb sysadmin boulder requested alternative overwhelming interesting \n",
      "1\t0.00864\tbryn vilka den det \n",
      "2\t0.0229\treligion belief messiah koresh \n",
      "3\t0.02754\tuniverse malaysia asks belief supernatural disk reasoning alternative \n",
      "4\t0.01138\tyalanci vay org reference columbia dbd hovig peace criminals sub \n",
      "5\t0.04705\tbank palestinian molestation meaningless runner reds conservative overwhelming org pouring female \n",
      "6\t0.04057\tdealer processor courier modems centris goer solun floppies \n",
      "7\t0.03228\tboone international straight interesting barry sox azerbaijan magazine \n",
      "8\t0.00933\tteel eclipse ida newsgroups comments \n",
      "9\t0.01522\tpray lunch esther piece sub peace female \n",
      "10\t0.0142\tmin det tor cgy umd \n",
      "11\t0.00861\tfredericton binghamton cup \n",
      "12\t0.01433\tspirit oneness concept trinitarianism substance approach arius biblical apologists plurality theology canon iii supernatural comments \n",
      "13\t0.0073\thell atheists survey abuse \n",
      "14\t0.01749\tcongress higher arguments reasoning bush current record somebody abuse powers piece \n",
      "\n",
      "[beta: 0.00804] \n",
      "<900> LL/token: -3.85245\n",
      "<910> LL/token: -3.85469\n",
      "<920> LL/token: -3.84377\n",
      "<930> LL/token: -3.85143\n",
      "<940> LL/token: -3.87606\n",
      "<950> LL/token: -3.85587\n",
      "<960> LL/token: -3.8606\n",
      "<970> LL/token: -3.85824\n",
      "<980> LL/token: -3.84783\n",
      "<990> LL/token: -3.87919\n",
      "\n",
      "0\t0.00843\tajteel barb sysadmin newsgroups alternative \n",
      "1\t0.00674\tbryn vilka den \n",
      "2\t0.01436\treligion belief messiah koresh \n",
      "3\t0.02169\tuniverse supernatural malaysia asks belief reasoning \n",
      "4\t0.00744\tyalanci vay org reference columbia dbd hovig peace criminals sub \n",
      "5\t0.0386\tbank palestinian molestation meaningless runner reds conservative female overwhelming disk \n",
      "6\t0.03168\tdealer processor courier modems centris canon goer floppies iii \n",
      "7\t0.02552\tboone international straight interesting barry sox survey azerbaijan magazine \n",
      "8\t0.00843\tmon teel eclipse ida boulder requested overwhelming interesting comments \n",
      "9\t0.01021\tpray lunch piece esther sub pouring peace \n",
      "10\t0.01255\tdet min tor cgy umd \n",
      "11\t0.00565\tfredericton binghamton cup \n",
      "12\t0.01209\tspirit oneness concept trinitarianism substance approach arius biblical apologists plurality theology comments \n",
      "13\t0.00619\thell atheists \n",
      "14\t0.01531\tcongress higher arguments reasoning bush current abuse record somebody powers solun \n",
      "\n",
      "[beta: 0.00747] \n",
      "<1000> LL/token: -3.8684\n",
      "\n",
      "Total time: 0 seconds\n"
     ]
    }
   ],
   "source": [
    "!mallet train-topics --input 20news-corpus.mallet --num-topics {num_topics} --num-iterations {num_iters} \\\n",
    "--output-doc-topics 20news-doc-topics.txt \\\n",
    "--output-topic-keys 20news-topic-keys.txt \\\n",
    "--word-topic-counts-file 20news-word-topic-counts-file.txt \\\n",
    "--topic-word-weights-file 20news-topic-word-weights-file.txt \\\n",
    "--xml-topic-report 20news-topic-report.xml \\\n",
    "--xml-topic-phrase-report 20news-topic-phrase-report.xml \\\n",
    "--show-topics-interval {show_interval} \\\n",
    "--use-symmetric-alpha false  \\\n",
    "--optimize-interval 100 \\\n",
    "--diagnostics-file 20news-diagnostics.xml\n",
    "# LL = log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "789px",
    "left": "0px",
    "right": "1186px",
    "top": "111px",
    "width": "254px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
