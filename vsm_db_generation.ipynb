{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YbadDvzWW0Sw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import sqlite3\n",
    "import os\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3prN-_cQ-8qd"
   },
   "source": [
    "# PART 1: Import and Preprocess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "2K22-bh4W488",
    "outputId": "b567da86-a5a6-4672-f895-86208b480480"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/leonardramsey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/leonardramsey/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leonardramsey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     /Users/leonardramsey/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leonardramsey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('tagsets')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AwIc1D23UuOQ"
   },
   "source": [
    "## Revised import function\n",
    "\n",
    "Using NLTK for parsing has improved the results of POS tagging. We do this for 10 different texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "JkFioCMVW8pl"
   },
   "outputs": [],
   "source": [
    "OHCO = ['chap_num', 'para_num', 'sent_num', 'token_num']\n",
    "CHAPS = OHCO[:1]\n",
    "PARAS = OHCO[:2]\n",
    "SENTS = OHCO[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Oxgp19ejZmU"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: NLTK tokenization messes up whitespace and \n",
    "handles non-alpha characters differently.\n",
    "\"\"\"\n",
    "\n",
    "def text_to_tokens(\n",
    "                   src_file,\n",
    "                   body_start=0, \n",
    "                   body_end=-1, \n",
    "                   chap_pat=r'^\\s*Chapter.*$', \n",
    "                   para_pat=r'\\n\\n+', \n",
    "                   sent_pat=r'([.;?!\"“”]+)', \n",
    "                   token_pat=r'([\\W_]+)'):\n",
    "\n",
    "    # Text to lines\n",
    "    lines = open(src_file, 'r', encoding='utf-8').readlines()\n",
    "    lines = lines[body_start - 1 : body_end + 1]\n",
    "    df = pd.DataFrame({'line_str':lines})\n",
    "    df.index.name = 'line_id'\n",
    "    del(lines)\n",
    "    \n",
    "    # FIX CHARACTERS TO IMPROVE TOKENIZATION\n",
    "    df.line_str = df.line_str.str.replace('—', ' — ')\n",
    "    df.line_str = df.line_str.str.replace('-', ' - ')\n",
    "\n",
    "    # Lines to Chapters\n",
    "    mask = df.line_str.str.match(chap_pat)\n",
    "    df.loc[mask, 'chap_id'] = df.apply(lambda x: x.name, 1)\n",
    "    df.chap_id = df.chap_id.ffill().astype('int')\n",
    "    chap_ids = df.chap_id.unique().tolist()\n",
    "    df['chap_num'] = df.chap_id.apply(lambda x: chap_ids.index(x))\n",
    "    chaps = df.groupby('chap_num')\\\n",
    "        .apply(lambda x: ''.join(x.line_str))\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'chap_str'})\n",
    "    del(df)\n",
    "\n",
    "    # Chapters to Paragraphs\n",
    "    paras = chaps.chap_str.str.split(para_pat, expand=True)\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'para_str'})\n",
    "    paras.index.names = PARAS\n",
    "    paras.para_str = paras.para_str.str.strip()\n",
    "    paras.para_str = paras.para_str.str.replace(r'\\n', ' ')\n",
    "    paras.para_str = paras.para_str.str.replace(r'\\s+', ' ')\n",
    "    paras = paras[~paras.para_str.str.match(r'^\\s*$')]\n",
    "    del(chaps)\n",
    "\n",
    "    # Paragraphs to Sentences\n",
    "#     sents = paras.para_str.str.split(sent_pat, expand=True)\\\n",
    "    sents = paras.para_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.sent_tokenize(x)))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'sent_str'})\n",
    "    sents.index.names = SENTS\n",
    "    del(paras)\n",
    "\n",
    "    # Sentences to Tokens\n",
    "#     tokens = sents.sent_str.str.split(token_pat, expand=True)\\\n",
    "    tokens = sents.sent_str\\\n",
    "        .apply(lambda x: pd.Series(nltk.pos_tag(nltk.word_tokenize(x))))\\\n",
    "        .stack()\\\n",
    "        .to_frame()\\\n",
    "        .rename(columns={0:'pos_tuple'})\n",
    "    tokens.index.names = OHCO\n",
    "    del(sents)\n",
    "    \n",
    "    tokens['pos'] = tokens.pos_tuple.apply(lambda x: x[1])\n",
    "    tokens['token_str'] = tokens.pos_tuple.apply(lambda x: x[0])\n",
    "    tokens = tokens.drop('pos_tuple', 1)\n",
    "\n",
    "    # Tag punctuation and numbers\n",
    "    tokens['punc'] = tokens.token_str.str.match(r'^[\\W_]*$').astype('int')\n",
    "    tokens['num'] = tokens.token_str.str.match(r'^.*\\d.*$').astype('int')\n",
    "    \n",
    "    # Extract vocab with minimal normalization\n",
    "    WORDS = (tokens.punc == 0) & (tokens.num == 0)\n",
    "    tokens.loc[WORDS, 'term_str'] = tokens.token_str.str.lower()\\\n",
    "        .str.replace(r'[\"_*.]', '')\n",
    "    \n",
    "    vocab = tokens[tokens.punc == 0].term_str.value_counts().to_frame()\\\n",
    "        .reset_index()\\\n",
    "        .rename(columns={'index':'term_str', 'term_str':'n'})\n",
    "    vocab = vocab.sort_values('term_str').reset_index(drop=True)\n",
    "    vocab.index.name = 'term_id'\n",
    "    \n",
    "    # Get priors for V\n",
    "    vocab['p'] = vocab.n / vocab.n.sum()\n",
    "    \n",
    "    # Add stems\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    vocab['port_stem'] = vocab.term_str.apply(lambda x: stemmer.stem(x))\n",
    "    \n",
    "    # Define stopwords\n",
    "    sw = pd.DataFrame({'x':1}, index=nltk.corpus.stopwords.words('english'))\n",
    "    vocab['stop'] = vocab.term_str.map(sw.x).fillna(0).astype('int')\n",
    "    del(sw)\n",
    "            \n",
    "    # Add term_ids to tokens \n",
    "    tokens['term_id'] = tokens['term_str'].map(vocab.reset_index()\\\n",
    "        .set_index('term_str').term_id).fillna(-1).astype('int')\n",
    "\n",
    "    return tokens, vocab\n",
    "\n",
    "def get_docs(tokens, div_names, doc_str = 'term_id', sep='', flatten=False, \n",
    "             index_only=False):\n",
    "    \n",
    "    if not index_only:\n",
    "        docs = tokens.groupby(div_names)[doc_str]\\\n",
    "          .apply(lambda x: x.str.cat(sep=sep))\n",
    "        docs.columns = ['doc_content']\n",
    "    else:\n",
    "        docs = tokens.groupby(div_names)[doc_str].apply(lambda x: x.tolist())\n",
    "\n",
    "    if flatten:\n",
    "        docs = docs.reset_index().drop(div_names, 1)\n",
    "    \n",
    "    return docs\n",
    "\n",
    "def get_term_id(vocab, term_str):\n",
    "    return vocab[vocab.term_str == term_str].index[0]\n",
    "\n",
    "def get_term_str(vocab, term_id):\n",
    "    return vocab.loc[term_id].term_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8kdbCiDyXmIK"
   },
   "outputs": [],
   "source": [
    "# src_file_name = '2701-0.txt'\n",
    "# src_file_url = 'https://www.gutenberg.org/files/2701/2701-0.txt'\n",
    "\n",
    "K_list = []\n",
    "V_list = []\n",
    "\n",
    "src_file_names = [\n",
    "                 'corpus/a_tale_of_two_cities.txt',\n",
    "                 'corpus/anna_karenina.txt',\n",
    "                 'corpus/captains_courageous.txt',\n",
    "                 'corpus/emma.txt',\n",
    "                 'corpus/far_from_madding_crowd.txt',\n",
    "                 'corpus/heart_of_darkness.txt',\n",
    "                 'corpus/jane_eyre.txt',\n",
    "                 'corpus/pride_and_prejudice.txt',\n",
    "                 'corpus/portrait_of_a_lady_vol1.txt',\n",
    "                 'corpus/portrait_of_a_lady_vol2.txt'\n",
    "                 ]\n",
    "\n",
    "db_file_names = [\n",
    "                 'db/a_tale_of_two_cities.db',\n",
    "                 'db/anna_karenina.db',\n",
    "                 'db/captains_courageous.db',\n",
    "                 'db/emma.db',\n",
    "                 'db/far_from_madding_crowd.db',\n",
    "                 'db/heart_of_darkness.db',\n",
    "                 'db/jane_eyre.db',\n",
    "                 'db/pride_and_prejudice.db',\n",
    "                 'db/portrait_of_a_lady_vol1.db',\n",
    "                 'db/portrait_of_a_lady_vol2.db'\n",
    "                 ]\n",
    "body_starts = [\n",
    "               111, # a tale of 2 cities\n",
    "               61, # anna karenina\n",
    "               38, # captains courageous\n",
    "               43, # emma\n",
    "               112, # far from madding crowd\n",
    "               38, # heart of darkness\n",
    "               72, # jane eyre\n",
    "               98, # pride and predjudice\n",
    "               43, # portrait of a lady (vol 1)\n",
    "               43 # portrait of a lady (vol 2)\n",
    "               ]\n",
    "body_ends = [\n",
    "            15900, # a tale of 2 cities\n",
    "            39892, # anna karenina\n",
    "            6185, # captains courageous\n",
    "            16262, # emma\n",
    "            17196, # far from madding crowd\n",
    "            3343, # heart of darkness\n",
    "            20700, # jane eyre\n",
    "            13339, # pride and predjudice\n",
    "            12879, # portrait of a lady (vol 1)\n",
    "            12221 # portrait of a lady (vol 2)\n",
    "            ]\n",
    "chap_pats = [\n",
    "            r'^\\s*(?:CHAPTER).*$', # a tale of 2 cities\n",
    "            r'^\\s*(?:Chapter|ETYMOLOGY|Epilogue).*$', # anna karenina\n",
    "            r'^\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$', # captains courageous\n",
    "            r'^\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$', # emma\n",
    "            r'^\\s*(?:CHAPTER|PREFACE).*$', # far from madding crowd\n",
    "            r'^\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$', # heart of darkness\n",
    "            r'^\\s*(?:CHAPTER|PREFACE|Epilogue).*$', # jane eyre\n",
    "            r'^\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$', # pride and predjudice\n",
    "            r'^\\s*(?:CHAPTER|PREFACE|Epilogue).*$', # portrait of a lady (vol 1)\n",
    "            r'^\\s*(?:CHAPTER|PREFACE|Epilogue).*$' # portrait of a lady (vol 2)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "src_file_name corpus/a_tale_of_two_cities.txt\n",
      "body_start 111\n",
      "body_end 15900\n",
      "chap_pat ^\\s*(?:CHAPTER).*$\n",
      "{'src_file': 'corpus/a_tale_of_two_cities.txt', 'body_start': 111, 'body_end': 15900, 'chap_pat': '^\\\\s*(?:CHAPTER).*$'}\n",
      "iteration 1\n",
      "src_file_name corpus/anna_karenina.txt\n",
      "body_start 61\n",
      "body_end 39892\n",
      "chap_pat ^\\s*(?:Chapter|ETYMOLOGY|Epilogue).*$\n",
      "{'src_file': 'corpus/anna_karenina.txt', 'body_start': 61, 'body_end': 39892, 'chap_pat': '^\\\\s*(?:Chapter|ETYMOLOGY|Epilogue).*$'}\n",
      "iteration 2\n",
      "src_file_name corpus/captains_courageous.txt\n",
      "body_start 38\n",
      "body_end 6185\n",
      "chap_pat ^\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$\n",
      "{'src_file': 'corpus/captains_courageous.txt', 'body_start': 38, 'body_end': 6185, 'chap_pat': '^\\\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$'}\n",
      "iteration 3\n",
      "src_file_name corpus/emma.txt\n",
      "body_start 43\n",
      "body_end 16262\n",
      "chap_pat ^\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$\n",
      "{'src_file': 'corpus/emma.txt', 'body_start': 43, 'body_end': 16262, 'chap_pat': '^\\\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$'}\n",
      "iteration 4\n",
      "src_file_name corpus/far_from_madding_crowd.txt\n",
      "body_start 112\n",
      "body_end 17196\n",
      "chap_pat ^\\s*(?:CHAPTER|PREFACE).*$\n",
      "{'src_file': 'corpus/far_from_madding_crowd.txt', 'body_start': 112, 'body_end': 17196, 'chap_pat': '^\\\\s*(?:CHAPTER|PREFACE).*$'}\n",
      "iteration 5\n",
      "src_file_name corpus/heart_of_darkness.txt\n",
      "body_start 38\n",
      "body_end 3343\n",
      "chap_pat ^\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$\n",
      "{'src_file': 'corpus/heart_of_darkness.txt', 'body_start': 38, 'body_end': 3343, 'chap_pat': '^\\\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$'}\n",
      "iteration 6\n",
      "src_file_name corpus/jane_eyre.txt\n",
      "body_start 72\n",
      "body_end 20700\n",
      "chap_pat ^\\s*(?:CHAPTER|PREFACE|Epilogue).*$\n",
      "{'src_file': 'corpus/jane_eyre.txt', 'body_start': 72, 'body_end': 20700, 'chap_pat': '^\\\\s*(?:CHAPTER|PREFACE|Epilogue).*$'}\n",
      "iteration 7\n",
      "src_file_name corpus/pride_and_prejudice.txt\n",
      "body_start 98\n",
      "body_end 13339\n",
      "chap_pat ^\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$\n",
      "{'src_file': 'corpus/pride_and_prejudice.txt', 'body_start': 98, 'body_end': 13339, 'chap_pat': '^\\\\s*(?:CHAPTER|ETYMOLOGY|Epilogue).*$'}\n",
      "iteration 8\n",
      "src_file_name corpus/portrait_of_a_lady_vol1.txt\n",
      "body_start 43\n",
      "body_end 12879\n",
      "chap_pat ^\\s*(?:CHAPTER|PREFACE|Epilogue).*$\n",
      "{'src_file': 'corpus/portrait_of_a_lady_vol1.txt', 'body_start': 43, 'body_end': 12879, 'chap_pat': '^\\\\s*(?:CHAPTER|PREFACE|Epilogue).*$'}\n",
      "iteration 9\n",
      "src_file_name corpus/portrait_of_a_lady_vol2.txt\n",
      "body_start 43\n",
      "body_end 12221\n",
      "chap_pat ^\\s*(?:CHAPTER|PREFACE|Epilogue).*$\n",
      "{'src_file': 'corpus/portrait_of_a_lady_vol2.txt', 'body_start': 43, 'body_end': 12221, 'chap_pat': '^\\\\s*(?:CHAPTER|PREFACE|Epilogue).*$'}\n"
     ]
    }
   ],
   "source": [
    "for s in range(0,len(src_file_names)):\n",
    "    print('iteration %d' % s)\n",
    "    print('src_file_name %s' % src_file_names[s])\n",
    "    print('body_start %d' % body_starts[s])\n",
    "    print('body_end %d' % body_ends[s])\n",
    "    print('chap_pat %s' % chap_pats[s])\n",
    "    cfg = dict(\n",
    "        src_file = src_file_names[s],\n",
    "        body_start = body_starts[s],\n",
    "        body_end = body_ends[s],\n",
    "        chap_pat = chap_pats[s]\n",
    "    )\n",
    "    print(cfg)\n",
    "\n",
    "    K, V = text_to_tokens(**cfg)\n",
    "    K_list.append(K)\n",
    "    V_list.append(V)\n",
    "    K_list[s].head()\n",
    "    V_list[s].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 2: Add Term Frequencies and Weights to Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean(row):\n",
    "    D1 = TFIDF.loc[row.name[0]]\n",
    "    D2 = TFIDF.loc[row.name[1]]\n",
    "    x = (D1 - D2)**2\n",
    "    y = x.sum() \n",
    "    z = np.sqrt(y)\n",
    "    return z\n",
    "\n",
    "def cosine(row):\n",
    "    D1 = TFIDF.loc[row.name[0]]\n",
    "    D2 = TFIDF.loc[row.name[1]]\n",
    "    x = D1 * D2\n",
    "    y = x.sum()\n",
    "    a = np.sqrt(D1.sum()**2)\n",
    "    b = np.sqrt(D2.sum()**2)\n",
    "    c = np.sqrt(a) * np.sqrt(b)\n",
    "    z = y / c\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5D0JAAQpx5O8"
   },
   "source": [
    "## Create DTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ax8ePBkk4qP"
   },
   "outputs": [],
   "source": [
    "# Create word mask\n",
    "\n",
    "# Let's filter out stopwords -- another hyperparameter. \n",
    "for db_file_index in range(0, len(db_file_names)): \n",
    "    WORDS = (K_list[db_file_index].punc == 0) & (K_list[db_file_index].num == 0) & K_list[db_file_index].term_id.isin(V_list[db_file_index][V_list[db_file_index].stop==0].index)\n",
    "\n",
    "    # Extrct BOW from tokens\n",
    "\n",
    "    # To extract a bag-of-words model from our tokens table, we apply a simple `groupby()` operation. Note that we can drop in our hyperparameters easily -- CHAPS and 'term_id' and be replaced. We can easily write a function to simplify this process and make it more configurable. \n",
    "\n",
    "    BOW = K_list[db_file_index][WORDS].groupby(OHCO[:1]+['term_id'])['term_id'].count()\n",
    "\n",
    "    ### Convert BOW to DTM\n",
    "\n",
    "    DTM = BOW.unstack().fillna(0)\n",
    "\n",
    "    ## ----- Compute Term Frequencies and Weights -----\n",
    "\n",
    "    ### Compute TF\n",
    "\n",
    "    alpha = .000001 # We introduce an arbitrary smoothing value\n",
    "    alpha_sum = alpha * V.shape[0]\n",
    "    TF = DTM.apply(lambda x: (x + alpha) / (x.sum() + alpha_sum), axis=1)\n",
    "\n",
    "    ### Compute TFIDF\n",
    "\n",
    "    N_docs = DTM.shape[0]\n",
    "    V_list[db_file_index]['df'] = DTM[DTM > 0].count()\n",
    "    TFIDF = TF * np.log2(N_docs / V_list[db_file_index][V_list[db_file_index].stop==0]['df'])\n",
    "\n",
    "    ### Compute TFTH (Experiment)\n",
    "\n",
    "    THM = -(TF * np.log2(TF))\n",
    "    TFTH = TF.apply(lambda x: x * THM.sum(), 1)\n",
    "\n",
    "    ### Add stats to V\n",
    "\n",
    "    V_list[db_file_index]['tf_sum'] = TF.sum()\n",
    "    V_list[db_file_index]['tf_mean'] = TF.mean()\n",
    "    V_list[db_file_index]['tf_max'] = TF.max()\n",
    "    V_list[db_file_index]['tfidf_sum'] = TFIDF.sum()\n",
    "    V_list[db_file_index]['tfidf_mean'] = TFIDF.mean()\n",
    "    V_list[db_file_index]['tfidf_max'] = TFIDF.max()\n",
    "    V_list[db_file_index]['tfth_sum'] = TFTH.sum()\n",
    "    V_list[db_file_index]['tfth_mean'] = TFTH.mean()\n",
    "    V_list[db_file_index]['tfth_max'] = TFTH.max()\n",
    "    V_list[db_file_index]['th_sum'] = THM.sum()\n",
    "    V_list[db_file_index]['th_mean'] = THM.mean()\n",
    "    V_list[db_file_index]['th_max'] = THM.max()\n",
    "\n",
    "    ## Create Docs table\n",
    "\n",
    "    D = DTM.sum(1).astype('int').to_frame().rename(columns={0:'term_count'})\n",
    "    D['tf'] = D.term_count / D.term_count.sum()\n",
    "\n",
    "    ## Get all doc pairs\n",
    "\n",
    "    chap_ids = D.index.tolist()\n",
    "    pairs = [(i,j) for i in chap_ids for j in chap_ids if j > i]\n",
    "    P = pd.DataFrame(pairs).reset_index(drop=True).set_index([0,1])\n",
    "    P.index.names = ['doc_x','doc_y']\n",
    "\n",
    "    ## Compute Euclidean distance\n",
    "\n",
    "    P['euclidean'] = 0\n",
    "    P['euclidean'] = P.apply(euclidean, 1)\n",
    "\n",
    "    ## Compute Cosine similarity\n",
    "\n",
    "    P['cosine'] = P.apply(cosine, 1)\n",
    "    \n",
    "    # Save data\n",
    "    if not os.path.exists('db/'):\n",
    "        os.makedirs('db/')\n",
    "        \n",
    "    with sqlite3.connect(db_file_names[db_file_index]) as db:\n",
    "        V_list[db_file_index].to_sql('vocab', db, if_exists='replace', index=True)\n",
    "        K_list[db_file_index].to_sql('token', db, if_exists='replace', index=True)\n",
    "        D.to_sql('doc', db, if_exists='replace', index=True)\n",
    "        P.to_sql('docpair', db, if_exists='replace', index=True)\n",
    "    #     BOW.to_frame().rename(columns={'term_id':'n'}).to_sql('bow', db, if_exists='replace', index=True)\n",
    "        TFIDF.stack().to_frame().rename(columns={0:'term_weight'})\\\n",
    "            .to_sql('dtm_tfidf', db, if_exists='replace', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DS5559_VSM.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
